{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc3073e1-a1e8-452f-bcc6-20466ba8e747",
   "metadata": {},
   "source": [
    "# Running a Test Suite on an External Function\n",
    "\n",
    "## Context\n",
    "Vellum Test Suites provide a framework for performing quantiative evaluation on AI applications at scale. You can use them to measure the quality of Prompts, Workflows, and even custom functions defined outside of Vellum in your codebase!\n",
    "\n",
    "This example details how to use Vellum Test Suites to run evals on an external function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb623fd-55fb-4f9f-8cd9-c095d2bef043",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "1. A Vellum account\n",
    "2. A Vellum API key, which can be created at [https://app.vellum.ai/api-keys](https://app.vellum.ai/api-keys)\n",
    "3. Install the `vellum-ai` pip package. We'll also use the getpass package in this notebook to store your Vellum API key.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8ea10c-d5f0-4557-972b-7f9aa293ad1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://_json_key_base64:****@us-central1-python.pkg.dev/vocify-prod/vocify/simple/\n",
      "Requirement already satisfied: vellum-ai in /Users/noaflaherty/Repos/vellum-ai/vellum-client-python/venv/lib/python3.11/site-packages (0.5.0)\n",
      "\u001B[31mERROR: Could not find a version that satisfies the requirement getpass (from versions: none)\u001B[0m\u001B[31m\n",
      "\u001B[0m\u001B[31mERROR: No matching distribution found for getpass\u001B[0m\u001B[31m\n",
      "\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install vellum-ai getpass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f61c3873-bf84-4a61-b1a0-36089704c360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "VELLUM_API_KEY = getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffb358-dd73-4ca0-bf5d-ccbee73e1b7e",
   "metadata": {},
   "source": [
    "## Test Suite Set Up\n",
    "To run evals on your external function, you must first configure a Test Suite through the Vellum web application at [https://app.vellum.ai/test-suites](https://app.vellum.ai/test-suites).\n",
    "\n",
    "Note that the Test Suite's \"Execution Interface\" must match that of the function that you'd like to evaluate. For example, if your function looks like:\n",
    "\n",
    "```python\n",
    "def my_function(arg_1: str, arg_2: str) -> str:\n",
    "    pass\n",
    "```\n",
    "\n",
    "Then you will want your Test Suite's Execution interface to look like this:\n",
    "![Test Suite Execution Interface](images/test-suite-execution-interface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d9f53-f57d-4db9-9a11-06fb6f689a99",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Now that everything is set up, it's time to write some code! First, we need to define the function whose output we want to evaluate. Here's how we can actually invoke the Test Suite against our function.\n",
    "\n",
    "Here we're using a Vellum Workflow as an example, but this code could do anything, including calling a Prompt Chain made via another\n",
    "third-party library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46070a6d-c661-49ea-b15d-0123fc8c8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vellum.types.named_test_case_variable_value_request import NamedTestCaseVariableValueRequest, NamedTestCaseStringVariableValueRequest\n",
    "from vellum.types.test_case_variable_value import TestCaseVariableValue\n",
    "\n",
    "def external_execution(inputs: list[TestCaseVariableValue]) -> list[NamedTestCaseVariableValueRequest]:\n",
    "    output_value = \"\".join([variable.value for variable in inputs])\n",
    "    output = NamedTestCaseStringVariableValueRequest(\n",
    "        type=\"STRING\",\n",
    "        value=output_value,\n",
    "        name=\"output\"\n",
    "    )\n",
    "    return [output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95384013-0636-42db-8d33-4f641d4d7e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 9580c9c2-ed5d-4206-a103-d6e487f6a54b\n"
     ]
    }
   ],
   "source": [
    "TEST_SUITE_ID = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57b72cff-0c3e-4fe5-ab7a-7f03b4bf04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vellum.client import Vellum\n",
    "from vellum.lib.test_suites import VellumTestSuite\n",
    "\n",
    "\n",
    "# Create a new VellumTestSuite object\n",
    "client = Vellum(api_key=VELLUM_API_KEY)\n",
    "test_suite = VellumTestSuite(test_suite_id=TEST_SUITE_ID, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831c1fd7-2f48-49ae-a941-3f0e3a721987",
   "metadata": {},
   "source": [
    "## Running Evals\n",
    "\n",
    "Here is where we actually trigger the Test Suite and pass in our executable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421c76de-20f9-4b11-bd4e-f6787148d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the external execution\n",
    "results = test_suite.run_external(executable=external_execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd38cf7-78bf-4cf6-996d-31f22530efa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TestSuiteRunMetricNumberOutput(value=1.0, name='score', type='NUMBER'),\n",
       " TestSuiteRunMetricNumberOutput(value=0.0, name='score', type='NUMBER'),\n",
       " TestSuiteRunMetricNumberOutput(value=1.0, name='score', type='NUMBER')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter down to a specific metric and a specific output that it produces.\n",
    "results.get_metric_outputs(\"Exact Match\", \"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aaa843-af86-48bd-9329-ced9280c740b",
   "metadata": {},
   "source": [
    "## Operating on the Results\n",
    "\n",
    "Above we use the`get_metric_outputs` function to retrieve all `score`'s for the `Exact Match` output.\n",
    "\n",
    "Note that under the hood, this function calls `wait_until_complete` to wait until the Test Suite Run has finished running.\n",
    "You can also call this function explicitly if you like ahead of time.\n",
    "\n",
    "`get_metric_outputs` is the primary way to interact with the outputs of a specified metric. With it, you can\n",
    "perform a variety of assertions to enforce whatever quality thresholds you like.\n",
    "\n",
    "If you want to operate directly on the raw executions for ultimate flexibility, use `results.all_executions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98609c5-754e-4e45-9155-f9a5e52f20e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do all Test Cases pass? No\n",
      "66.66666666666666% of Test Cases pass. Acceptable? Yes\n",
      "Is the average score acceptable? Yes\n",
      "Is the minimum score acceptable? No\n",
      "Is the maximum regressing? No\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[VellumTestSuiteRunExecution(id='95431327-d864-4cc1-bd47-50430c39ebeb', test_case_id='99971a73-429d-4a28-9003-afbe5cadb868', outputs=[TestSuiteRunExecutionOutput_String(name='output', value='Hello, world!', output_variable_id='c3f48fd5-6df7-4116-bd69-fb624d8d7d88', type='STRING')], metric_results=[TestSuiteRunExecutionMetricResult(metric_id='c4ac96a5-2101-4e1e-8dfb-3fccdc1ebde0', outputs=[TestSuiteRunMetricNumberOutput(value=1.0, name='score', type='NUMBER'), TestSuiteRunMetricNumberOutput(value=1.0, name='normalized_score', type='NUMBER')], metric_label='Exact Match', metric_definition=TestSuiteRunExecutionMetricDefinition(id='9a8a4c32-0258-41be-beac-063628fe50e6', label='Exact Match', name='exact-match'))]),\n",
       " VellumTestSuiteRunExecution(id='363833cf-4b0f-4993-876f-ed578d792414', test_case_id='3fdb81b7-2147-42c8-92b2-b8f322ad9853', outputs=[TestSuiteRunExecutionOutput_String(name='output', value='Failingtest', output_variable_id='c3f48fd5-6df7-4116-bd69-fb624d8d7d88', type='STRING')], metric_results=[TestSuiteRunExecutionMetricResult(metric_id='c4ac96a5-2101-4e1e-8dfb-3fccdc1ebde0', outputs=[TestSuiteRunMetricNumberOutput(value=0.0, name='score', type='NUMBER'), TestSuiteRunMetricNumberOutput(value=0.0, name='normalized_score', type='NUMBER')], metric_label='Exact Match', metric_definition=TestSuiteRunExecutionMetricDefinition(id='9a8a4c32-0258-41be-beac-063628fe50e6', label='Exact Match', name='exact-match'))]),\n",
       " VellumTestSuiteRunExecution(id='21ed2cb5-9d48-44e8-9b58-59bcfe9ef505', test_case_id='d4e6885c-4d10-4099-bc2f-9e8dff37c4c2', outputs=[TestSuiteRunExecutionOutput_String(name='output', value='Goodbye cruel, world...', output_variable_id='c3f48fd5-6df7-4116-bd69-fb624d8d7d88', type='STRING')], metric_results=[TestSuiteRunExecutionMetricResult(metric_id='c4ac96a5-2101-4e1e-8dfb-3fccdc1ebde0', outputs=[TestSuiteRunMetricNumberOutput(value=1.0, name='score', type='NUMBER'), TestSuiteRunMetricNumberOutput(value=1.0, name='normalized_score', type='NUMBER')], metric_label='Exact Match', metric_definition=TestSuiteRunExecutionMetricDefinition(id='9a8a4c32-0258-41be-beac-063628fe50e6', label='Exact Match', name='exact-match'))])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def print_result(msg: str, result: bool) -> None:\n",
    "    print(msg, \"Yes\" if result else \"No\")\n",
    "\n",
    "# Example of asserting that every Test Cases passes\n",
    "all_test_cases_pass = all([result.value == 1.0 for result in results.get_metric_outputs(\"exact-match\", \"score\")])\n",
    "print_result(\"Do all Test Cases pass?\", all_test_cases_pass)\n",
    "\n",
    "# Example asserting that at least 50% of results have a score above a specified threshold\n",
    "num_test_cases_passing = results.get_count_metric_outputs(\"exact-match\", \"score\", predicate=lambda x: x.value >= 0.5)\n",
    "num_test_cases_total = results.get_count_metric_outputs(\"exact-match\", \"score\")\n",
    "percent_test_cases_passing = num_test_cases_passing / num_test_cases_total\n",
    "print_result(f\"{percent_test_cases_passing * 100}% of Test Cases pass. Acceptable?\", percent_test_cases_passing > 0.5)\n",
    "\n",
    "# Example of asserting that the average score is greater than a specified threshold\n",
    "avg_score_acceptable = results.get_mean_metric_output(\"exact-match\", \"score\") > 0.5\n",
    "print_result(\"Is the average score acceptable?\", avg_score_acceptable)\n",
    "\n",
    "# Example of asserting that the min score is greater than a specified threshold\n",
    "min_score_acceptable = results.get_min_metric_output(\"exact-match\", \"score\") > 0.5\n",
    "print_result(\"Is the minimum score acceptable?\", min_score_acceptable)\n",
    "\n",
    "# Example of asserting that the max score is greater than a specified threshold\n",
    "max_score_acceptable = results.get_min_metric_output(\"exact-match\", \"score\") > 0.75\n",
    "print_result(\"Is the maximum regressing?\", max_score_acceptable)\n",
    "\n",
    "# Print out all results\n",
    "results.all_executions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
